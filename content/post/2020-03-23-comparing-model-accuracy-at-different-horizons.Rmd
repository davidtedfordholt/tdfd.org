---
title: “Comparing Model Accuracy at Different Horizons”
author: “David Holt”
date: “3/19/2020”
output: html_document
theme: tufte_html
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(forecast)
library(fable)
library(fabletools)
library(feasts)
library(tsibble)
library(tsibbledata)
library(tsbox)
```

## Time series of interest

First and foremost, I love the `tsibble` package. It’s the most sensible thing to happen to time series analysis in R since, well, `forecast`.
Additionally, I love `tsbox` for making my <insert time series format> into a tsibbles.

```{r data}
wine <- forecast::wineind %>%
    tsbox::ts_tsibble() %>%
    tsibble::index_by(month = tsibble::yearmonth(time)) %>%
    tsibble::update_tsibble(index = month) %>%
    dplyr::select(-time) %>%
    ungroup()
```

## Examine time series

I love `feasts`, for all the time series-specific plotting.

```{r plots1}
feasts::autoplot(wine, value) +
    labs(title = "Wine Sales in Australian", 
         x = "Month", 
         y = "Total Sales") +
    theme_minimal()
```

## Model time series

I also love `fable`, for bringing forecasting into the world of tidy data. Look at all the models!

```{r model}
models <-
    wine %>%
    fabletools::model(
        mean = fable::MEAN(value)
        ,rw_drift = fable::RW(value ~ drift())
        ,naive = fable::NAIVE(value)
        ,snaive = fable::SNAIVE(value)
        ,ets = fable::ETS(value)
        ,arima = fable::ARIMA(value)
    )
models
```

## Forecast time series

`fable` makes it just as easy to forecast all those models, too. Let’s forecast 6 months!

```{r forecast}
forecasts <- fabletools::forecast(models, h = 6)
forecasts
```

## Plot forecasts

It just keeps getting better...

```{r plot_forecasts}
feasts::autoplot(forecasts, wine) +
    ggplot2::lims(x = c(as.Date("1993-01-01"), NA)) +
    ggplot2::facet_wrap(~ .model, ncol = 2) +
    labs(title = "Model Predictions", 
         x = "Month", 
         y = "Total Sales") +
    theme_minimal()
```

## Test model fit

Getting fit statistics from the models is a breeze.

It looks like ETS and ARIMA are the clear winners here...

```{r fit}
accuracy <- fabletools::accuracy(models)
accuracy
```

## Test model performance on a holdout

Of course, we don’t just want to look at fit. We like rigor. We demand a holdout.
... still easy!

```{r holdout}
models_holdout <-
    wine %>%
    filter(month < as.Date("1994-03-01")) %>%
    fabletools::model(
        mean = fable::MEAN(value)
        ,rw_drift = fable::RW(value ~ drift())
        ,naive = fable::NAIVE(value)
        ,snaive = fable::SNAIVE(value)
        ,ets = fable::ETS(value)
        ,arima = fable::ARIMA(value)
    ) 

forecasts_holdout <-
    models_holdout %>%
    fabletools::forecast(h = 6)

forecasts_holdout %>%
    feasts::autoplot(wine) +
    ggplot2::lims(x = c(as.Date("1993-01-01"), NA)) +
    ggplot2::facet_wrap(~ .model, ncol = 2) +
    labs(title = "Model Performance on Holdout", 
         x = "Month", 
         y = "Total Sales") +
    theme_minimal()
```

## Calculating accuracy on a holdout

```{r holdout_accuracy}
accuracy_holdout <- fabletools::accuracy(forecasts_holdout, wine)

accuracy_holdout
```

## Compare accuracy between training and holdout

We can see, and this isn't too surprising, that the three simplest methods (mean, naive and random walk with drift) all failed to fit the training data as tightly, but did similarly well on the holdout.

ARIMA and the mean performed best on the holdout sample.

```{r compare_accuracy}
accuracy %>%
    left_join(accuracy_holdout, by = ".model", suffix = c(".train", ".holdout")) %>%
    ggplot2::ggplot() +
    ggplot2::geom_point(aes(x = MAPE.holdout, y = MAPE.train, color = .model)) +
    ggplot2::geom_abline(slope = 1, intercept = c(0,0)) +
    ggplot2::lims(x = c(0, 25), y = c(0, 25)) +
    labs(
        title = "Accuracy in Training Fit and Holdout Performance", 
        x = "Holdout MAPE", 
        y = "Training MAPE")
```

## Compare the bias

The underprediction of the naive and random walk models is obvious, as is the overprediction of the seasonal naive.

The mean, followed by ETS, gives the least biased predictions on the holdout sample.

```{r compare_bias}
accuracy %>%
    left_join(accuracy_holdout, by = ".model", suffix = c(".train", ".holdout")) %>%
    ggplot2::ggplot() +
    ggplot2::geom_point(aes(x = MPE.holdout, y = MPE.train, color = .model)) +
    ggplot2::geom_hline(yintercept = 0) +
    ggplot2::geom_vline(xintercept = 0) +
    ggplot2::lims(x = c(-10, 12), y = c(-10, 12)) +
    labs(
        title = "Bias in Training Fit and Holdout Performance", 
        x = "Holdout Mean Percent Error", 
        y = "Training Mean Percent Error")
```

## Intermediate Conclusion

When we apply the rigor of a holdout, maybe the mean is the way to go. In terms of accuracy, the mean was nearly as good as ARIMA. In terms of bias, the mean was the best. Of course, it's possible that this is just because of the particular 6 months we looked at.

If we really want to know, we will need to try a few different holdouts. Time series cross validation, ladies and gentlemen!

```{r tscv}
model_at_cutoff <-
    function(cutoff_date, tsbl) {
        tsbl %>%
            tsibble::filter_index(~ cutoff_date) %>%
            fabletools::model(
                mean = fable::MEAN(value)
                ,rw_drift = fable::RW(value ~ drift())
                ,naive = fable::NAIVE(value)
                ,snaive = fable::SNAIVE(value)
                ,ets = fable::ETS(value)
                ,arima = fable::ARIMA(value)
                ,var = fable::VAR(value)
                ,nnetar = fable::NNETAR(value ~ AR(period = "1 year"))
            ) %>%
            # dplyr::mutate(
            #     ensemble_all = (mean + rw_drift + naive + snaive + ets + arima) / 6
            #     ,ensemble_seasonal = (snaive + ets + arima) / 3
            #     ,ensemble_naive = (mean + rw_drift + naive + snaive) / 4
            # ) %>%
            fabletools::forecast(h = 6) %>%
            tsibble::as_tsibble() %>%
            dplyr::select(-.distribution) %>%
            dplyr::rename(forecast = value) %>%
            dplyr::left_join(wine, by = "month") %>%
            dplyr::mutate(
                cutoff = cutoff_date
                ,residual = value - forecast
                ,percent_residual = residual / value * 100
                ,horizon = month - cutoff
            ) %>%
            tibble::as_tibble()
    }

cutoff_dates <- wine$month[nrow(wine)-36:6]

cv <-
    cutoff_dates %>%
    purrr::map_df(model_at_cutoff, tsbl = wine) %>%
    dplyr::mutate(month = tsibble::yearmonth(month)) %>%
    tsibble::as_tsibble(
        key = c(".model", "horizon"),
        index = month
    )

cv_accuracy <-
    cv %>%
    tibble::as_tibble() %>%
    dplyr::group_by(.model, horizon) %>%
    dplyr::summarise(
        MAPE = mean(abs(percent_residual))
        ,MPE = mean(percent_residual)
    )

cv_accuracy %>%
    ggplot(aes(x = horizon, y = MAPE, color = .model)) +
    geom_line() +
    lims(x = c(0, NA), y = c(0, NA)) +
    labs(title = "MAPE by horizon", 
         x = "Horizon", 
         y = "MAPE") +
    theme_minimal()

cv_accuracy %>%
    ggplot(aes(x = MPE, y = MAPE, color = horizon)) +
    geom_point() +
    lims(x = c(-10, 10), y = c(0, NA)) +
    ggplot2::geom_hline(yintercept = 0) +
    ggplot2::geom_vline(xintercept = 0) +
    facet_wrap(~ .model) +
    labs(title = "MAPE by horizon", 
         x = "MPE", 
         y = "MAPE") +
    theme_minimal()
```

## The "Real" World

But let's ask how we would actually USE the information about wine sales in Australia by month.

## The Wine Merchant
(How cool is the phrase "wine merchant"?)

Business Decision: how much wine to purchase to bring my stock up to what I'll need this month